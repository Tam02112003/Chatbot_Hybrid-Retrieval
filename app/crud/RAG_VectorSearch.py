from langchain_community.vectorstores import FAISS
from app.pkgs.model import call_gemma3n_api
from app.upload_data import embedding_model



# T√¨m ki·∫øm theo semantic
def semantic_search(question: str, top_k: int = 3):
    vectorstore = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)
    return vectorstore.similarity_search(question, k=top_k)

# H·ªèi v√† k·∫øt h·ª£p th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi
def ask_rag_question(question: str):
    semantic_results = semantic_search(question)

    context_chunks = "\n".join(
        f"- {doc.page_content[:300]}" for doc in semantic_results
    ) if semantic_results else "(Kh√¥ng c√≥ ng·ªØ c·∫£nh ph√π h·ª£p)"

    prompt = f"""
Ng∆∞·ªùi d√πng h·ªèi: "{question}"

C√°c ƒëo·∫°n vƒÉn li√™n quan t·ª´ semantic search:
{context_chunks}

H√£y tr·∫£ l·ªùi m·ªôt c√°ch th√¢n thi·ªán nh∆∞ m·ªôt tr·ª£ l√Ω ·∫£o c·ªßa c√¥ng ty HDBank.
""".strip()

    try:
        return "üí¨ " + call_gemma3n_api(prompt).strip()
    except Exception as e:
        print(f"‚ùå L·ªói g·ªçi LLM: {e}")
        return context_chunks
